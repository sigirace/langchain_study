## Memory

📌 **모든 memory의 공통 api**

- save_context: Save context from this conversation to buffer.
- load_memory_variables: Return history buffer.

### 5.0 ConversationBufferMemory

- openai api는 memory를 지원하지 않음 ☞ 현재는 어떤지 모름
- langchain은 메모리를 통해 실제 대화의 느낌을 지원

📍 **ConversationBufferMemory**

> Buffer for storing conversation memory.

- 이전 대화 내용 전체를 기억함
- 내용이 길어질 수록 메모리 낭비가 커져서 비효율적

### 5.1 ConversationBufferWindowMemory

📍 **ConversationBufferWindowMemory**

> Buffer for storing conversation memory inside a limited size window.

- _parameters_
  - k: 버퍼 윈도우의 사이즈, 몇개의 메세지를 저장할지를 뜻함

### 5.2 ConversationSummaryMemory

📍 **ConversationSummaryMemory**

> Conversation summarizer to chat memory.

- llm을 사용하여 대화를 요약함

📌 **ConversationSummaryMemory와 메모리 관계**

- 대화가 없는 초반 차지하는 메모리가 상대적으로 큼
- 대화가 길어질수록 차지하는 메모리는 상대적으로 작음

### 5.3 ConversationSummaryBufferMemory

📍 **ConversationSummaryBufferMemory**

> Buffer with summarizer for storing conversation memory.

- ConversationBufferMemory, ConversationSummaryMemory를 결합
- 메모리에 보내온 메세지의 수를 저장
- limit에 다른 순간에 오래된 메세지를 요약함
- 즉, 가장 최근의 상호 작용을 계속 추적하며 과거는 요약으로 가지고 있음
- _parameter_
  - max_token_limit: 요약되기 전 가능한 메시지 토큰 수의 최대값

### 5.4 ConversationKGMemory

> Knowledge graph conversation memory.

- 질문에 대해 엔티티를 추출해서 Knowledge Graph를 생성함
- 외부 지식 그래프와 통합하여 대화에서 지식 트리플에 대한 정보를 저장하고 검색

❤️‍🔥 **Memory Types**

📜 [공식문서-메모리 타입](https://python.langchain.com/v0.1/docs/modules/memory/types/)

### 5.5 Memory on LLMChain

📍 **LLM Chain**

- LangChain 라이브러리에서 제공하는 고수준의 추상화로 LLM 기반의 체인을 생성하고 관리하는 데 사용됨
- LLMChain은 주로 프롬프트 템플릿과 언어 모델을 결합하여 일관된 방식으로 입력을 처리하고, 모델의 출력을 얻기 위해 사용됨
- _parameters_
  - llm: Large Language Model
  - memory: buffer
  - prompt: prompt
  - verbose: logs

👀 **off-the-shelf**

- 일반적인 목적을 가진 chain (llm chain)
- 반대 개념은 custom chain

💡 **해석**

- 앞서 chain을 만들때, interface 규칙에 맞춰 커스텀으로 구성했음
- LLMchain을 쓰면 이 내용은 고민하지 않고 Args만 전달하면 됨

📌 **LLMChain에서 프롬프트에 메모리를 적용하는 방법**

- step1. Memory 생성
  - 모든 Memory는 memory_key를 가지고 있음
- step2. String 기반의 template를 만듦
  - template안에는 memory_key의 변수명으로 과거 내용을 담을 수 있는 placeholder 존재
- step3. LLMChain 인스턴스 생성
  - LLMChain 내에서 memory와 template의 공통된 memory_key를 연결함

### 5.6 Chat Based Memory

- Memory를 단순 text가 아닌 Chat Message 형태로 저장
- _parameters_
  - return_messages=True

📍 **MessagesPlaceholder**

> Prompt template that assumes variable is already list of messages.

- 과거 대화(messages)로부터 ChatPromptTemplate을 만들어야함
- 그러나 과거 대화(messages)가 몇개인지 알 수 없음
- 얼마만큼의 공간을 할당해야할지 모를때 `MessagePlaceholder`를 사용
- _parameters_
  - variable_name: message_key

### 5.7 LCEL Based Memory

⛔️ **Error**

```python
llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=50,
    memory_key="chat_history",
    return_messages=True,
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a chatbot that helps people with their daily life."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}"),
])

chain = prompt | llm

chain.invoke({"question": "My name is Nico"})
```

- 해당 방식으로 수행한다면 Prompt에 들어가는 chat_history에 대한 내용이 없음
- LLMChain에서는 framework가 이를 자동화해주었지만, custom chain을 사용할 경우 이에 대한 처리 필요

💥 **해결방안 1**

```python
chain.invoke({
    "chat_history": memory.load_memory_variables({})["chat_history"],
    "question": "My name is Nico"
})
```

- prompt의 input으로 chat_history key 추가

💥 **해결방안 2**

```python
from langchain.schema.runnable import RunnablePassthrough

def load_memory():
    return memory.load_memory_variables({})['chat_history']

chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm
```

- chain 실행시, 가장 먼저 load_memory 함수를 호출함
- 단, memory에 대해서 데이터를 쌓는 save_context에 대한 수동 구현 필요

```python
def invoke_chain(question):
    result = chain.invoke({"question": question})
    memory.save_context(
        {"input": question},
        {"output": result.content},
    )
```

📍 **RunnablePassthrough**

- 객체는 input_data를 받아서 그대로 output_data로 반환함
- 주로 데이터 파이프라인에서 중간 단계로 사용되며, 특정 단계에서 데이터를 변경하지 않고 그대로 전달해야 할 때 유용함

🌈 **example**

```python
def load_memory(input):
    return memory.load_memory_variables({})['chat_history']

chain = RunnablePassthrough.assign(chat_history=load_memory)

print(chain.invoke({"question": "My name is Nico"}))
```

```
{'question': 'My name is Nico', 'chat_history': []}
```

### 6.1 Data Loaders and Splitters

📜 [공식문서-Retrival](https://python.langchain.com/v0.1/docs/modules/data_connection/)

📍 **UnstructureFileLoader**

- 다양한 파일들을 가져올 수 있음
- pdf, docx, excel, html ... even ppt!

📌 **loader의 output**

```python
print(loader.load())
print(len(loader.load()))
```

```
[Document(page_content='~~~~~')]
1
```

- Document 객체의 List인데 하나로 모두 포함되어 있음
  ☞ text를 나누어줘야 할 필요가 있음

👀 **chunk**

- chunk: 문서가 나누어진 단위
- chunk_size: 글자 수
  - chunk_size를 측정하는 기본적인 방법은 python의 len function

📍 **RecursiveCharacterTextSplitter**

- 문장 끝이나 문단의 끝부분마다 끊어줌
- 문장 중간에서 짤려 의미를 잃어버리는 것을 방지
- _parameters_
  - chunk_size: 문단의 사이즈
  - chunk_overlap: 앞 조각의 끝을 조금 가져와 문장을 연결시킴

📍 **CharacterTextSplitter**

- _parameters_
  - chunk_size, chunk_overlap
  - splitter: 문단을 splite

⛔️ **CharacterTextSplitter는 내 생각과 다르게 분할한다!**

📜 [Stackoverflow CharacterTextSplitter](https://stackoverflow.com/questions/76633836/what-does-langchain-charactertextsplitters-chunk-size-param-even-do)

- CharacterTextSplitter는 구분 기호(기본값은 '\n\n')에서만 분할됨
- chunk_size는 분할이 가능한 경우 분할할 최대 청크 크기
- 문자열이 n개의 문자로 시작하고, 구분 기호가 있으며, 다음 구분 기호 앞에 m개의 문자가 더 있는 경우 첫 번째 청크 크기는 chunk_size < n + m + len(separator)이면 n이 됨

🌈 **Example**

**Case1**

```python
from langchain.text_splitter import CharacterTextSplitter

# "\n\n"를 구분자로 설정하여 텍스트를 분할
splitter = CharacterTextSplitter(
    separator=r"\n\n",
    is_separator_regex=True,
    chunk_size=12,
    chunk_overlap=0,
)

# 예제 텍스트 설정
text = """Part 1, Chapter 1\n\nPart One\n\n1\n\nIt was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his"""

# 텍스트 분할 수행
split_texts = splitter.split_text(text)

# 분할된 청크의 수 출력
print(len(split_texts))

# 분할된 청크 출력
for i, chunk in enumerate(split_texts):
    print(f"Chunk {i + 1}:\n{chunk}\n")
```

```
Created a chunk of size 17, which is longer than the specified 12
4
Chunk 1:
Part 1, Chapter 1

Chunk 2:
Part One

Chunk 3:
1

Chunk 4:
It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his
```

- chunk_size (12) < n+m+len(sep) (13) ☞ True
  - n: 8 (Part One)
  - m: 1 (1)
  - len(sep): 4
- chunk_size가 n까지

**Case2**

```python
splitter = CharacterTextSplitter(
    separator=r"\n\n",
    is_separator_regex=True,
    chunk_size=13,
    chunk_overlap=0,
)

# 텍스트 분할 수행
split_texts = splitter.split_text(text)

# 분할된 청크의 수 출력
print(len(split_texts))

# 분할된 청크 출력
for i, chunk in enumerate(split_texts):
    print(f"Chunk {i + 1}:\n{chunk}\n")
```

```
Created a chunk of size 17, which is longer than the specified 13
3
Chunk 1:
Part 1, Chapter 1

Chunk 2:
Part One\n\n1

Chunk 3:
It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his
```

- chunk_size (13) < n+m+len(sep) (13) ☞ false
  - n: 8 (Part One)
  - m: 1 (1)
  - len(sep): 4
- chunk_size가 n이 아니라 다음 분할 까지 포함 (구분자도 포함됨)

### 6.2 Tiktoken

🖥️ [OpenAI Tokenizer](https://platform.openai.com/tokenizer)

📍 **tiktoken**

> Text splitter that uses tiktoken encoder to count length. made by openAI

- OpenAI의 토크나이저로 분할하기에 모델과 lanchain 내에서 텍스트를 카운팅하는 방법이 일치하게 됨

### 6.3 Vectors

🖥️ [Word to Vec](https://turbomaze.github.io/word2vecjson/)

### 6.4 Vector Store

📌 **embed_query의 과정**

- Step1. 토큰화 ☞ tiktoken
- Step2. 임베딩 ☞ model api
- Step3. 집계 ☞ 벡터 평균의 정규화

📍 **vectorstore**

> 벡터 공간에서 검색을 할 수 있게 하는 데이터 베이스

📍 **LocalFileStore**

> BaseStore interface that works on the local file system.

📍 **CacheBackedEmbeddings**

> Interface for caching results from embedding models.

- from_bytes_store: 캐시에 임베딩이 존재하는지 확인하고
  - 없다면 openai embedding
  - 있다면 cached embedding (vector store)

### 6.5 Langsmith

👀 **Langsmith**

> api 사용 분석

```
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT='https://api.smith.langchain.com'
LANGCHAIN_API_KEY='API_KEY'
```
