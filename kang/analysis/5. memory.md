## Memory

ğŸ“Œ **ëª¨ë“  memoryì˜ ê³µí†µ api**

- save_context: Save context from this conversation to buffer.
- load_memory_variables: Return history buffer.

### 5.0 ConversationBufferMemory

- openai apiëŠ” memoryë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ â˜ í˜„ì¬ëŠ” ì–´ë–¤ì§€ ëª¨ë¦„
- langchainì€ ë©”ëª¨ë¦¬ë¥¼ í†µí•´ ì‹¤ì œ ëŒ€í™”ì˜ ëŠë‚Œì„ ì§€ì›

ğŸ“ **ConversationBufferMemory**

> Buffer for storing conversation memory.

- ì´ì „ ëŒ€í™” ë‚´ìš© ì „ì²´ë¥¼ ê¸°ì–µí•¨
- ë‚´ìš©ì´ ê¸¸ì–´ì§ˆ ìˆ˜ë¡ ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ì»¤ì ¸ì„œ ë¹„íš¨ìœ¨ì 

### 5.1 ConversationBufferWindowMemory

ğŸ“ **ConversationBufferWindowMemory**

> Buffer for storing conversation memory inside a limited size window.

- _parameters_
  - k: ë²„í¼ ìœˆë„ìš°ì˜ ì‚¬ì´ì¦ˆ, ëª‡ê°œì˜ ë©”ì„¸ì§€ë¥¼ ì €ì¥í• ì§€ë¥¼ ëœ»í•¨

### 5.2 ConversationSummaryMemory

ğŸ“ **ConversationSummaryMemory**

> Conversation summarizer to chat memory.

- llmì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìš”ì•½í•¨

ğŸ“Œ **ConversationSummaryMemoryì™€ ë©”ëª¨ë¦¬ ê´€ê³„**

- ëŒ€í™”ê°€ ì—†ëŠ” ì´ˆë°˜ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ê°€ ìƒëŒ€ì ìœ¼ë¡œ í¼
- ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‘ìŒ

### 5.3 ConversationSummaryBufferMemory

ğŸ“ **ConversationSummaryBufferMemory**

> Buffer with summarizer for storing conversation memory.

- ConversationBufferMemory, ConversationSummaryMemoryë¥¼ ê²°í•©
- ë©”ëª¨ë¦¬ì— ë³´ë‚´ì˜¨ ë©”ì„¸ì§€ì˜ ìˆ˜ë¥¼ ì €ì¥
- limitì— ë‹¤ë¥¸ ìˆœê°„ì— ì˜¤ë˜ëœ ë©”ì„¸ì§€ë¥¼ ìš”ì•½í•¨
- ì¦‰, ê°€ì¥ ìµœê·¼ì˜ ìƒí˜¸ ì‘ìš©ì„ ê³„ì† ì¶”ì í•˜ë©° ê³¼ê±°ëŠ” ìš”ì•½ìœ¼ë¡œ ê°€ì§€ê³  ìˆìŒ
- _parameter_
  - max_token_limit: ìš”ì•½ë˜ê¸° ì „ ê°€ëŠ¥í•œ ë©”ì‹œì§€ í† í° ìˆ˜ì˜ ìµœëŒ€ê°’

### 5.4 ConversationKGMemory

> Knowledge graph conversation memory.

- ì§ˆë¬¸ì— ëŒ€í•´ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•´ì„œ Knowledge Graphë¥¼ ìƒì„±í•¨
- ì™¸ë¶€ ì§€ì‹ ê·¸ë˜í”„ì™€ í†µí•©í•˜ì—¬ ëŒ€í™”ì—ì„œ ì§€ì‹ íŠ¸ë¦¬í”Œì— ëŒ€í•œ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰

â¤ï¸â€ğŸ”¥ **Memory Types**

ğŸ“œ [ê³µì‹ë¬¸ì„œ-ë©”ëª¨ë¦¬ íƒ€ì…](https://python.langchain.com/v0.1/docs/modules/memory/types/)

### 5.5 Memory on LLMChain

ğŸ“ **LLM Chain**

- LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ê³ ìˆ˜ì¤€ì˜ ì¶”ìƒí™”ë¡œ LLM ê¸°ë°˜ì˜ ì²´ì¸ì„ ìƒì„±í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë¨
- LLMChainì€ ì£¼ë¡œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ì–¸ì–´ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ëª¨ë¸ì˜ ì¶œë ¥ì„ ì–»ê¸° ìœ„í•´ ì‚¬ìš©ë¨
- _parameters_
  - llm: Large Language Model
  - memory: buffer
  - prompt: prompt
  - verbose: logs

ğŸ‘€ **off-the-shelf**

- ì¼ë°˜ì ì¸ ëª©ì ì„ ê°€ì§„ chain (llm chain)
- ë°˜ëŒ€ ê°œë…ì€ custom chain

ğŸ’¡ **í•´ì„**

- ì•ì„œ chainì„ ë§Œë“¤ë•Œ, interface ê·œì¹™ì— ë§ì¶° ì»¤ìŠ¤í…€ìœ¼ë¡œ êµ¬ì„±í–ˆìŒ
- LLMchainì„ ì“°ë©´ ì´ ë‚´ìš©ì€ ê³ ë¯¼í•˜ì§€ ì•Šê³  Argsë§Œ ì „ë‹¬í•˜ë©´ ë¨

ğŸ“Œ **LLMChainì—ì„œ í”„ë¡¬í”„íŠ¸ì— ë©”ëª¨ë¦¬ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•**

- step1. Memory ìƒì„±
  - ëª¨ë“  MemoryëŠ” memory_keyë¥¼ ê°€ì§€ê³  ìˆìŒ
- step2. String ê¸°ë°˜ì˜ templateë¥¼ ë§Œë“¦
  - templateì•ˆì—ëŠ” memory_keyì˜ ë³€ìˆ˜ëª…ìœ¼ë¡œ ê³¼ê±° ë‚´ìš©ì„ ë‹´ì„ ìˆ˜ ìˆëŠ” placeholder ì¡´ì¬
- step3. LLMChain ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  - LLMChain ë‚´ì—ì„œ memoryì™€ templateì˜ ê³µí†µëœ memory_keyë¥¼ ì—°ê²°í•¨

### 5.6 Chat Based Memory

- Memoryë¥¼ ë‹¨ìˆœ textê°€ ì•„ë‹Œ Chat Message í˜•íƒœë¡œ ì €ì¥
- _parameters_
  - return_messages=True

ğŸ“ **MessagesPlaceholder**

> Prompt template that assumes variable is already list of messages.

- ê³¼ê±° ëŒ€í™”(messages)ë¡œë¶€í„° ChatPromptTemplateì„ ë§Œë“¤ì–´ì•¼í•¨
- ê·¸ëŸ¬ë‚˜ ê³¼ê±° ëŒ€í™”(messages)ê°€ ëª‡ê°œì¸ì§€ ì•Œ ìˆ˜ ì—†ìŒ
- ì–¼ë§ˆë§Œí¼ì˜ ê³µê°„ì„ í• ë‹¹í•´ì•¼í• ì§€ ëª¨ë¥¼ë•Œ `MessagePlaceholder`ë¥¼ ì‚¬ìš©
- _parameters_
  - variable_name: message_key

### 5.7 LCEL Based Memory

â›”ï¸ **Error**

```python
llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=50,
    memory_key="chat_history",
    return_messages=True,
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a chatbot that helps people with their daily life."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}"),
])

chain = prompt | llm

chain.invoke({"question": "My name is Nico"})
```

- í•´ë‹¹ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤ë©´ Promptì— ë“¤ì–´ê°€ëŠ” chat_historyì— ëŒ€í•œ ë‚´ìš©ì´ ì—†ìŒ
- LLMChainì—ì„œëŠ” frameworkê°€ ì´ë¥¼ ìë™í™”í•´ì£¼ì—ˆì§€ë§Œ, custom chainì„ ì‚¬ìš©í•  ê²½ìš° ì´ì— ëŒ€í•œ ì²˜ë¦¬ í•„ìš”

ğŸ’¥ **í•´ê²°ë°©ì•ˆ 1**

```python
chain.invoke({
    "chat_history": memory.load_memory_variables({})["chat_history"],
    "question": "My name is Nico"
})
```

- promptì˜ inputìœ¼ë¡œ chat_history key ì¶”ê°€

ğŸ’¥ **í•´ê²°ë°©ì•ˆ 2**

```python
from langchain.schema.runnable import RunnablePassthrough

def load_memory():
    return memory.load_memory_variables({})['chat_history']

chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm
```

- chain ì‹¤í–‰ì‹œ, ê°€ì¥ ë¨¼ì € load_memory í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•¨
- ë‹¨, memoryì— ëŒ€í•´ì„œ ë°ì´í„°ë¥¼ ìŒ“ëŠ” save_contextì— ëŒ€í•œ ìˆ˜ë™ êµ¬í˜„ í•„ìš”

```python
def invoke_chain(question):
    result = chain.invoke({"question": question})
    memory.save_context(
        {"input": question},
        {"output": result.content},
    )
```

ğŸ“ **RunnablePassthrough**

- ê°ì²´ëŠ” input_dataë¥¼ ë°›ì•„ì„œ ê·¸ëŒ€ë¡œ output_dataë¡œ ë°˜í™˜í•¨
- ì£¼ë¡œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ì¤‘ê°„ ë‹¨ê³„ë¡œ ì‚¬ìš©ë˜ë©°, íŠ¹ì • ë‹¨ê³„ì—ì„œ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì „ë‹¬í•´ì•¼ í•  ë•Œ ìœ ìš©í•¨

ğŸŒˆ **example**

```python
def load_memory(input):
    return memory.load_memory_variables({})['chat_history']

chain = RunnablePassthrough.assign(chat_history=load_memory)

print(chain.invoke({"question": "My name is Nico"}))
```

```
{'question': 'My name is Nico', 'chat_history': []}
```
