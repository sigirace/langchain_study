## Memory

📌 **모든 memory의 공통 api**

- save_context: Save context from this conversation to buffer.
- load_memory_variables: Return history buffer.

### 5.0 ConversationBufferMemory

- openai api는 memory를 지원하지 않음 ☞ 현재는 어떤지 모름
- langchain은 메모리를 통해 실제 대화의 느낌을 지원

📍 **ConversationBufferMemory**

> Buffer for storing conversation memory.

- 이전 대화 내용 전체를 기억함
- 내용이 길어질 수록 메모리 낭비가 커져서 비효율적

### 5.1 ConversationBufferWindowMemory

📍 **ConversationBufferWindowMemory**

> Buffer for storing conversation memory inside a limited size window.

- _parameters_
  - k: 버퍼 윈도우의 사이즈, 몇개의 메세지를 저장할지를 뜻함

### 5.2 ConversationSummaryMemory

📍 **ConversationSummaryMemory**

> Conversation summarizer to chat memory.

- llm을 사용하여 대화를 요약함

📌 **ConversationSummaryMemory와 메모리 관계**

- 대화가 없는 초반 차지하는 메모리가 상대적으로 큼
- 대화가 길어질수록 차지하는 메모리는 상대적으로 작음

### 5.3 ConversationSummaryBufferMemory

📍 **ConversationSummaryBufferMemory**

> Buffer with summarizer for storing conversation memory.

- ConversationBufferMemory, ConversationSummaryMemory를 결합
- 메모리에 보내온 메세지의 수를 저장
- limit에 다른 순간에 오래된 메세지를 요약함
- 즉, 가장 최근의 상호 작용을 계속 추적하며 과거는 요약으로 가지고 있음
- _parameter_
  - max_token_limit: 요약되기 전 가능한 메시지 토큰 수의 최대값

### 5.4 ConversationKGMemory

> Knowledge graph conversation memory.

- 질문에 대해 엔티티를 추출해서 Knowledge Graph를 생성함
- 외부 지식 그래프와 통합하여 대화에서 지식 트리플에 대한 정보를 저장하고 검색

❤️‍🔥 **Memory Types**

📜 [공식문서-메모리 타입](https://python.langchain.com/v0.1/docs/modules/memory/types/)

### 5.5 Memory on LLMChain

📍 **LLM Chain**

- LangChain 라이브러리에서 제공하는 고수준의 추상화로 LLM 기반의 체인을 생성하고 관리하는 데 사용됨
- LLMChain은 주로 프롬프트 템플릿과 언어 모델을 결합하여 일관된 방식으로 입력을 처리하고, 모델의 출력을 얻기 위해 사용됨
- _parameters_
  - llm: Large Language Model
  - memory: buffer
  - prompt: prompt
  - verbose: logs

👀 **off-the-shelf**

- 일반적인 목적을 가진 chain (llm chain)
- 반대 개념은 custom chain

💡 **해석**

- 앞서 chain을 만들때, interface 규칙에 맞춰 커스텀으로 구성했음
- LLMchain을 쓰면 이 내용은 고민하지 않고 Args만 전달하면 됨

📌 **LLMChain에서 프롬프트에 메모리를 적용하는 방법**

- step1. Memory 생성
  - 모든 Memory는 memory_key를 가지고 있음
- step2. String 기반의 template를 만듦
  - template안에는 memory_key의 변수명으로 과거 내용을 담을 수 있는 placeholder 존재
- step3. LLMChain 인스턴스 생성
  - LLMChain 내에서 memory와 template의 공통된 memory_key를 연결함

### 5.6 Chat Based Memory

- Memory를 단순 text가 아닌 Chat Message 형태로 저장
- _parameters_
  - return_messages=True

📍 **MessagesPlaceholder**

> Prompt template that assumes variable is already list of messages.

- 과거 대화(messages)로부터 ChatPromptTemplate을 만들어야함
- 그러나 과거 대화(messages)가 몇개인지 알 수 없음
- 얼마만큼의 공간을 할당해야할지 모를때 `MessagePlaceholder`를 사용
- _parameters_
  - variable_name: message_key

### 5.7 LCEL Based Memory

⛔️ **Error**

```python
llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=50,
    memory_key="chat_history",
    return_messages=True,
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a chatbot that helps people with their daily life."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}"),
])

chain = prompt | llm

chain.invoke({"question": "My name is Nico"})
```

- 해당 방식으로 수행한다면 Prompt에 들어가는 chat_history에 대한 내용이 없음
- LLMChain에서는 framework가 이를 자동화해주었지만, custom chain을 사용할 경우 이에 대한 처리 필요

💥 **해결방안 1**

```python
chain.invoke({
    "chat_history": memory.load_memory_variables({})["chat_history"],
    "question": "My name is Nico"
})
```

- prompt의 input으로 chat_history key 추가

💥 **해결방안 2**

```python
from langchain.schema.runnable import RunnablePassthrough

def load_memory():
    return memory.load_memory_variables({})['chat_history']

chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm
```

- chain 실행시, 가장 먼저 load_memory 함수를 호출함
- 단, memory에 대해서 데이터를 쌓는 save_context에 대한 수동 구현 필요

```python
def invoke_chain(question):
    result = chain.invoke({"question": question})
    memory.save_context(
        {"input": question},
        {"output": result.content},
    )
```

📍 **RunnablePassthrough**

- 객체는 input_data를 받아서 그대로 output_data로 반환함
- 주로 데이터 파이프라인에서 중간 단계로 사용되며, 특정 단계에서 데이터를 변경하지 않고 그대로 전달해야 할 때 유용함

🌈 **example**

```python
def load_memory(input):
    return memory.load_memory_variables({})['chat_history']

chain = RunnablePassthrough.assign(chat_history=load_memory)

print(chain.invoke({"question": "My name is Nico"}))
```

```
{'question': 'My name is Nico', 'chat_history': []}
```
