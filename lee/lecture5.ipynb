{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain에는 5가지 종류의 메모리가 있다. 각자 저장 방법도 다르고, 장단점도 다르다.   \n",
    "챗봇에 메모리를 추가하지 않으면 아무것도 기억할 수 없기 때문에, 이어지는 질문을 해도 기억된 답변이 나올 수 없음. (stateless)   \n",
    "대화형 챗봇을 만들기 위해서는 대화 느낌을 줄 수 있는 MEMORY 사용이 필수적이다.   \n",
    "모델 자체에는 메모리가 없기 떄문에, 모델에게 요청을 보낼 땐 이전 대화 기록을 보내줘야 함.   \n",
    "\n",
    "메모리를 다루는 측면에서는 최대한 적은 대화 내용을 저장해 이후 답변 내용에 최대한 반영하는 것이 목표가 될 것임.\n",
    "\n",
    "### 1. ConversationBufferMemory\n",
    "- 단순히 이전 대화 내용 전체를 저장한다.\n",
    "- 대화가 길어질 수록 메모리 효율이 떨어짐\n",
    "\n",
    "### 2. ConversationBufferWindowMemory\n",
    "- ConverationBufferMemory가 모든 대화내용을 저장함에 따라 발생하는 메모리 비효율을 일부 완화\n",
    "- 저장량에 Limit을 두어, 모든 대화가 아닌 최근 대화만을 저장한다는 차이가 있음.\n",
    "- 즉, 메모리의 임계치를 넘지 않을 수 있다는 장점을 가짐,\n",
    "- 단 버퍼의 크기를 넘어 lost된 대화 내용은 기억할 수 없다는 단점이 있음. \n",
    "\n",
    "### 3. ConversationSummaryMemory (ChatOpenAI)\n",
    "- message를 그대로 저장하는 것이 아닌, conversation의 요약을 자체적으로 해 줌.\n",
    "- 매우 긴 Conversation이 있는 경우 유리함. \n",
    "- 초반에는 더 많은 토큰과 저장 공간이 필요해 짧은 대화에는 비효율적일 수 있음.\n",
    "\n",
    "### 4. ConversationSummaryBufferMemory (★)\n",
    "- ConversationSummary + ConversationBuffer\n",
    "- 메모리에 보내 온 메시지를 그대로 저장하며 그 수를 셈\n",
    "- 지정한 limit에 다다른 순간 ***오래된 메시지를 Summarize***함.\n",
    "\n",
    "### 5. ConversationKGMemory (KG : Knowledge Graph)\n",
    "- 대화 중 Entity의 KG를 만들어 가장 중요한 것들(로 판단되는 것들)을 요약\n",
    "- history가 아닌 ***Entity를 가지고 오기 때문에*** 예제의 get_history method는 사용하지 않았음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"]=\"./etc\"\n",
    "\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "## FewShotPromptTemplate 실습을 위해 import\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "## ConversationBuffer(Window)Memory\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "## ConversationSummaryMemory\n",
    "from langchain.memory import ConversationSummaryMemory, ConversationSummaryBufferMemory\n",
    "## ConversationKGMemory\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "## LCEL Based Memory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,     ##Streaming 옵션 ON\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## memory = ConversationBufferMemory()\n",
    "## memory = ConversationBufferMemory(return_messages=True)\n",
    "## memory = ConversationSummaryMemory(llm=chat)\n",
    "\n",
    "\"\"\"\n",
    "memory = ConversationKGMemory(llm=chat, return_messages=True)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input,}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    memory.load_memory_variables({})\n",
    "\"\"\"\n",
    "\n",
    "## memory에 현재까지의 대화 context를 저장 (수동)\n",
    "##memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "\n",
    "##memory.load_memory_variables({})\n",
    "\n",
    "## output ##\n",
    "## {'history': 'Human: Hi!\\nAI: How are you?'}\n",
    "## {'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]} -- return_messages=true\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, return_messages=True)\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBuffer의 결과.\n",
    "save_context를 통해 넣어 주었던 대화 내용이 string으로 출력된다.\n",
    "챗 모델을 사용하기 위해서는 AI/Human의 데이터가 각각 필요하지만 현재의 output 은 단순 string이므로,   \n",
    "챗봇 목적으로 메모리를 사용할 경우 처음 memory를 생성할 때 return_messages=True로 설정한다.      \n",
    "이 경우 HumanMessage, AIMessage가 분리되어 list형태로 반환한다.   \n",
    "이 과정을 반복하면 동일한 대화 내역이 계속 저장되어 메모리 비효율 발생   \n",
    "\n",
    "### 참고\n",
    "모든 메모리는 save_context, load_memory_variables 라는 함수를 가지고 있다.   \n",
    "즉 메모리의 종류만 결정한다면 아래 단의 코드 수정은 필요 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nadd_message(\"Hi, I\\'m Nicolas, I live in South Korea\", \"Wow that is so cool!\")\\nadd_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\\n##add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\\n\\n##get_history()\\n\\n## KG memory load\\nmemory.load_memory_variables({\"input\":\"who is Nicolas\"})\\n\\n## output (ConversationSummaryMemory)\\n## The human greets the AI with a \"Hi!\" and the AI responds by asking how the human is. \\n## The human introduces themselves as Nicolas from South Korea, and the AI responds by saying that it is cool. \\n## The human mentions that South Korea is so pretty, and the AI expresses a wish to go there.\\n\\n## output (ConversationKGMemory)\\n## (Nicolas, lives in, South Korea)Nicolas\\n## (Nicolas, lives in, South Korea)(Nicolas, likes, kimchi)Nicolas\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ConversationSummaryMemory\n",
    "\"\"\"\n",
    "add_message(\"Hi, I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "##add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "\n",
    "##get_history()\n",
    "\n",
    "## KG memory load\n",
    "memory.load_memory_variables({\"input\":\"who is Nicolas\"})\n",
    "\n",
    "## output (ConversationSummaryMemory)\n",
    "## The human greets the AI with a \"Hi!\" and the AI responds by asking how the human is. \n",
    "## The human introduces themselves as Nicolas from South Korea, and the AI responds by saying that it is cool. \n",
    "## The human mentions that South Korea is so pretty, and the AI expresses a wish to go there.\n",
    "\n",
    "## output (ConversationKGMemory)\n",
    "## (Nicolas, lives in, South Korea)Nicolas\n",
    "## (Nicolas, lives in, South Korea)(Nicolas, likes, kimchi)Nicolas\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummary의 결과\n",
    "대화 내용 자체를 저장하지 않고, ***대화에서 벌어지고 있는 상황에 대한 요약글***을 자체 제작해 저장함.   \n",
    "위 예제의 경우 add한 데이터의 양보다 Summary의 양이 더욱 많으나, 대화량이 많아질수록 Summarized text의 효율이 증가할 것임.\n",
    "\n",
    "\n",
    "* Issue\n",
    "HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url 에러 발생   \n",
    "인증서 확인 과정에서 문제가 발생한 것으로 보임   \n",
    "1. requests, certifi 업그레이드 => 해결 안됨\n",
    "2. requests, certifi 통해 url 검증 강제 False => 해결 안됨\n",
    "=> CA pem 파일의 문제인 것으로 보임, 실행하지 못했음\n",
    "\n",
    "★ tiktoken 사용 시 네트워크 환경으로 인한 SSL 오류로,   \n",
    "필요한 파일을 다운받아 로컬 캐시로 지정함\n",
    "\n",
    "(코드 맨 위의 os.verion[\"TIKTOKEN_CACHE_DIR\"]=\"./env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationKGMemory의 결과\n",
    "대화 내의 Entity 기반으로 내용을 저장함. 예제의 경우 'Nicolas' 라는 Entity에 대한 정보를 저장하였음.   \n",
    "* 그럼 이 정보들을 하나하나 다 입력해 주어야 하는가? (save_context, load_variables의 반복)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory on LLMChain\n",
    "- 메모리를 chain에 연결하는 방법과 두 종류의 chain을 사용하는 방법에 대해 설명   \n",
    "- LLMChain : off-the-shelf chain (일반적인 목적을 가진 chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "##ConversationSummaryBufferMemory 오류로 BufferMemory 사용\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=chat,\n",
    "    ##max_token_limit=80,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,   ##문자열이 아닌 Message로 반환\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "## PromptTemplate 사용 시\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "## ChatPromptTemplate 사용 시\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    ## 대화 저장 memory의 내용으로 MessagePlaceholder를 채움\n",
    "    ## Memory가 Sysetm/Human/AI Message를 주면 얼마나 올지 모르기에 placeholder에 담음\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),      ## 누구의, 얼마나 큰 메시지인지 알 수 없는 공간\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,   ## Chain의 Prompt log를 확인 가능\n",
    ")\n",
    "## LLMChain의 경우 위의 format으로만 구성이 가능함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\u001b[0m\n",
      "Nice to meet you, Nico! How can I assist you today?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "Your name is Nico.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 독립적인 질문이 후속 질문에 영향을 주지 못하고 있음.   \n",
    "우리가 Prompt에게 대화 내역을 말해준 적이 없기 떄문임. (Memory에게 History를 넘기는 것과는 별개임.)   \n",
    "(My name is Nico -> What is my name?)   \n",
    "\n",
    "Memory에는 대화 기록들이 정상적으로 저장되고 있지만, 이 데이터를 Prompt에게 넘겨주지를 못하는 상황   \n",
    "따라서 우리가 원하는 어떠한 방식으로 Prompt에게 대화 기록을 추가해 주어야 함   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='My name is Nico'),\n",
       "  AIMessage(content='Nice to meet you, Nico! How can I assist you today?'),\n",
       "  HumanMessage(content='I live in Seoul'),\n",
       "  AIMessage(content='Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or talk about regarding Seoul?'),\n",
       "  HumanMessage(content='What is my name?'),\n",
       "  AIMessage(content='Your name is Nico. How can I assist you further, Nico?')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Based Memory (LCEL : LangChain Expression Language)\n",
    "생성된 chain에 메모리를 추가하는 것은 어렵지 않음   \n",
    "\n",
    "* RunnablePassthrough\n",
    "- chain에서 prompt의 앞 순서에 실행되어 그 결과를 prompt에게 넘겨줌\n",
    "- prompt의 format 이전에 함수를 실행시킬 수 있어 변수 세팅에 용이함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you mentioned that your name is Nico. How can I assist you today, Nico?"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='Yes, you mentioned that your name is Nico. How can I assist you today, Nico?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\"\"\"\n",
    "chain = prompt | chat\n",
    "chain.invoke({\n",
    "    \"chat_history\":memory.load_memory_variables({})[\"chat_history\"],\n",
    "    \"question\": \"My name is Nico\"\n",
    "})\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## 이 코드는 위와 동일한 결과를 낸다.\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | chat\n",
    "chain.invoke({\n",
    "    \"question\": \"My name is Nico\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 각 결과를 메모리에 수동으로 저장하고 있는 점을 해결할 필요가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": \"My name is Nico\"\n",
    "    })\n",
    "    memory.save_context({\"input\":question}, {\"outpt\":result.content})\n",
    "    ##print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I understand that your name is Nico. How can I assist you today, Nico?content='Yes, I understand that your name is Nico. How can I assist you today, Nico?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got it, Nico! How can I assist you today?content='Got it, Nico! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : memory_key는 default값이 'history'이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
